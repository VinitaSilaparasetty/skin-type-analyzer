{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> Skin Type Analysis using Thermal Imaging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <center> By Vinita Silaparasetty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import dlib\n",
    "import cv2\n",
    "from collections import Counter\n",
    "from skimage.color import rgb2lab, deltaE_cie76\n",
    "import os\n",
    "%matplotlib inline\n",
    "from imutils import face_utils\n",
    "import argparse\n",
    "import imutils\n",
    "import dlib\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = cv2.imread('oily_skin.jpg')\n",
    "print(\"The type of this input is {}\".format(type(image)))\n",
    "print(\"Shape: {}\".format(image.shape))\n",
    "plt.imshow(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resize Image\n",
    "The standard size of an image taken on a smartphone is 640 pixels in width and 320 pixels in height.\n",
    "I have resized the image to reduce the time taken to analyze it.I did not make the image too small, as the original data will be lost and it will affect the accuracy of the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modified_image = cv2.resize(image, (600, 300), interpolation = cv2.INTER_AREA)\n",
    "modified_image = modified_image.reshape(modified_image.shape[0]*modified_image.shape[1], 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Facial Feature Identification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a dictionary for facial regions\n",
    "FACIAL_LANDMARKS_IDXS = OrderedDict([\n",
    "    (\"mouth\", (48, 68)),\n",
    "    (\"right_eyebrow\", (17, 22)),\n",
    "    (\"left_eyebrow\", (22, 27)),\n",
    "    (\"right_eye\", (36, 42)),\n",
    "    (\"left_eye\", (42, 48)),\n",
    "    (\"nose\", (27, 35)),\n",
    "    (\"jaw\", (0, 17))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct the argument parser and parse the arguments\n",
    "argp = argparse.ArgumentParser()\n",
    "argp.add_argument(\"-p\", \"--shape-predictor\", required=True,\n",
    " help=\"path to facial landmark predictor\")\n",
    "argp.add_argument(\"-i\", \"--image\", required=True,\n",
    " help=\"path to input image\")\n",
    "args = vars(argp.parse_args())\n",
    " \n",
    "# facial landmark predictor\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "predictor = dlib.shape_predictor(args[\"shape_predictor\"])\n",
    " \n",
    "# detect faces in the grayscale image\n",
    "rects = detector(gray, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop over the face detections\n",
    "for (i, rect) in enumerate(rects):\n",
    "    # determine the facial landmarks for the face region, then\n",
    "    # convert the landmark (x, y)-coordinates to a NumPy array\n",
    "    shape = predictor(gray, rect)\n",
    "    shape = face_utils.shape_to_np(shape)\n",
    " \n",
    "    # loop over the face parts individually\n",
    "    for (name, (i, j)) in face_utils.FACIAL_LANDMARKS_IDXS.items():\n",
    "    # clone the original image so we can draw on it, then\n",
    "    # display the name of the face part on the image\n",
    "        clone = image.copy()\n",
    "        cv2.putText(clone, name, (10, 30), cv2.FONT_HERSHEY_SIMPLEX,0.7, (0, 0, 255), 2)\n",
    " \n",
    "         # loop over the subset of facial landmarks, drawing the\n",
    "         # specific face part\n",
    "         for (x, y) in shape[i:j]:\n",
    "            cv2.circle(clone, (x, y), 1, (0, 0, 255), -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the ROI of the face region as a separate image\n",
    "        (x, y, w, h) = cv2.boundingRect(np.array([shape[i:j]]))\n",
    "         roi = image[y:y + h, x:x + w]\n",
    "         roi = imutils.resize(roi, width=250, inter=cv2.INTER_CUBIC)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Color Identification\n",
    "Convert greyscale image to color"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = cv2.cvtColor(gray,cv2.COLOR_GRAY2RGB)\n",
    "plt.imshow(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reverse the Colorscale\n",
    "CV2 reads colors in a numpy array in the reverse order. So we need to take the RGB scale and convert it to the BGR scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RGB2HEX(color):\n",
    "    return \"#{:02x}{:02x}{:02x}\".format(int(color[0]), int(color[1]), int(color[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check for warm regions which are in red or orange\n",
    "IMAGE_DIRECTORY = 'images'\n",
    "COLORS = {\n",
    "    'RED': [255, 0, 0],\n",
    "    'ORANGE':[255,165,0],\n",
    "}   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specify Oily Regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x == 48 and y == 68\n",
    " print(\"Mouth\")\n",
    "    for x == 17 and y == 22\n",
    "     print(\"Right Eyebrow\")\n",
    "        for x == 22 and y == 27\n",
    "         print(\"Left Eyebrow\")\n",
    "            for x == 36 and y == 42\n",
    "             print(\"Right eye\")\n",
    "                for x == 42 and y == 48\n",
    "                 print(\"Left eye\")\n",
    "                    for x == 27 and y == 35\n",
    "                     print(\"nose\")    \n",
    "                        for x == 0 and y == 17\n",
    "                         print(\"jaw\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
